{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5zDH4fIDYK5",
    "outputId": "f4c84983-fc3f-48fb-8058-29cb7ae3d373"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IhLQHMZhDYzn",
    "outputId": "81a9607b-7132-41f8-907b-a7a99dcd156c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "def cpu():\n",
    "  with tf.device('/cpu:0'):\n",
    "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
    "    return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "def gpu():\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "  \n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu()\n",
    "gpu()\n",
    "\n",
    "# Run the op several times.\n",
    "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
    "      '(batch x height x width x channel). Sum of ten runs.')\n",
    "print('CPU (s):')\n",
    "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
    "print(cpu_time)\n",
    "print('GPU (s):')\n",
    "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
    "print(gpu_time)\n",
    "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoUet-AwoWG5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tO3HWGsw4I8m"
   },
   "outputs": [],
   "source": [
    "def train_model(img_dir, preprocess):\n",
    "\n",
    "  data_dir = pathlib.Path(img_dir)\n",
    "\n",
    "  batch_size = 64\n",
    "  img_height = 128\n",
    "  img_width = 128\n",
    "\n",
    "  IMG_SHAPE = (img_height, img_width) + (3,)\n",
    "\n",
    "  train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "  val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "  class_names = train_ds.class_names\n",
    "\n",
    "  AUTOTUNE = tf.data.AUTOTUNE\n",
    "  num_classes = len(class_names)\n",
    "\n",
    "  train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "  val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "  data_augmentation = Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "  ])\n",
    "\n",
    "  # Create the base model from the pre-trained model\n",
    "  base_model = tf.keras.applications.DenseNet201(input_shape=IMG_SHAPE,\n",
    "                                              include_top=False,\n",
    "                                              weights='imagenet')\n",
    "  \n",
    "  base_model.trainable = False\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(128, 128, 3))\n",
    "  x = data_augmentation(inputs)\n",
    "\n",
    "  if preprocess == True:\n",
    "    x = tf.keras.applications.densenet.preprocess_input(x)\n",
    "    print(\"Using preprocessing\")\n",
    "\n",
    "  x = base_model(x)\n",
    "  x = tf.keras.layers.MaxPooling2D()(x)\n",
    "  x = tf.keras.layers.Dropout(0.2)(x)\n",
    "  x = tf.keras.layers.Flatten()(x)\n",
    "  x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "  x = tf.keras.layers.Dropout(0.2)(x)\n",
    "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "  outputs = tf.keras.layers.Dense(5)(x)\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  base_learning_rate = 0.0005\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "  epochs=100\n",
    "  history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs\n",
    "  )\n",
    "\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  epochs_range = range(epochs)\n",
    "\n",
    "  plt.figure(figsize=(16, 6))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "  plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.title('Training and Validation Accuracy')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(epochs_range, loss, label='Training Loss')\n",
    "  plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "  plt.legend(loc='upper right')\n",
    "  plt.title('Training and Validation Loss')\n",
    "  plt.show()\n",
    "    \n",
    "  return {'acc': acc, 'val_acc': val_acc, 'loss': loss, 'val_loss': val_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a6A0VoMm2ltm",
    "outputId": "e60cb1b7-892a-40b2-fd91-cfbb0dcfff4f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_hls_pre = train_model('content/img_HLS', preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AaZ99f642ndR",
    "outputId": "74751d47-8218-4cd3-d23f-0dd88703781b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_hls_nonpre = train_model('content/img_HLS', preprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_luv_pre = train_model('content/img_Luv', preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_luv_nonpre = train_model('content/img_Luv', preprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_ycrcb_pre = train_model('content/img_YCrCb', preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_ycrcb_nonpre = train_model('content/img_YCrCb', preprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res_hls_pre).to_csv(\"densenet201_nontrainable/res_hls_pre.csv\", index=False)\n",
    "pd.DataFrame(res_hls_nonpre).to_csv(\"densenet201_nontrainable/res_hls_nonpre.csv\", index=False)\n",
    "pd.DataFrame(res_luv_pre).to_csv(\"densenet201_nontrainable/res_luv_pre.csv\", index=False)\n",
    "pd.DataFrame(res_luv_nonpre).to_csv(\"densenet201_nontrainable/res_luv_nonpre.csv\", index=False)\n",
    "pd.DataFrame(res_ycrcb_pre).to_csv(\"densenet201_nontrainable/res_ycrcb_pre.csv\", index=False)\n",
    "pd.DataFrame(res_ycrcb_nonpre).to_csv(\"densenet201_nontrainable/res_ycrcb_nonpre.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOvD5haic2jmNQdbxg2jvku",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "All Color Space - VGG19 Non-trainable.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
